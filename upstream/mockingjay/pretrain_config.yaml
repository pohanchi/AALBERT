masking_strategy:
  mask_proportion: 0.15
  mask_consecutive: 1

optimizer:
  name: AdamW
  lr: 5.0e-5
  weight_decay: 1.0e-5

scheduler:
  name: linear_decay_schedule_with_warmup
  num_warmup_steps: 10000
  num_training_steps: 500000
  
datarc:
  file_path:
    Librispeech360: ../../librispeech/LibriSpeech/train-clean-360/ # key: corresponding data path
    # you can custom your key_name and offer the corresponding path  
  
  dataloader:
    batch_size: 24
    num_workers: 8
    pin_memory: True
  max_timestep: 160000

  input:
    feature_type: mel
    config_path: ./upstream/aalbert/input_config.yaml


  target:
    - feature_type: mel
      config_path: ./upstream/aalbert/input_config.yaml
    - feature_type: linear
      config_path: ./upstream/aalbert/target_config.yaml

loss_function: L1Loss
loss_config: 
  reduction: sum

trainer_config:
  accumulate_grad_batches: 1
  amp_level: "O1"
  gpus: '0'
  gradient_clip_val: 10.0
  max_steps: 500000
  log_every_n_steps: 1000
  benchmark: true
  flush_logs_every_n_steps: 1000
  deterministic: true
  weights_summary: "top"
  progress_bar_refresh_rate: 1
  profiler: "simple"
  process_position: 0


